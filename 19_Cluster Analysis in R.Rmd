---
title: "19_Cluster Analysis in R"
author: "Steve Chevalier"
date: "September 26, 2018"
output: html_document
---
Load data for this course
```{r}
library(readr)
con <- gzfile("./data/cluster_lineup.rds")
lineup <- readRDS(con)
close(con)

con <- gzfile("./data/cluster_oes.rds")
oes <- readRDS(con)
close(con)

con <- gzfile("./data/cluster_ws_customers.rds")
ws_customers <- readRDS(con)
close(con)
```

1 Calculating distance between observations
  Cluster analysis seeks to find groups of observations that are similar to one another, but the identified groups are different from each other. This similarity/difference is captured by the metric called distance. In this chapter, you will learn how to calculate the distance between observations for both continuous and categorical features. You will also develop an intuition for how the scales of your features can affect distance.

In which of these scenarios would clustering methods likely be appropriate?
1) Using consumer behavior data to identify distinct segments within a market.
2) Predicting whether a given user will click on an ad.
3) Identifying distinct groups of stocks that follow similar trading patterns.
4) Modeling & predicting GDP growth.
1 & 3 are correct

Market segmentation and pattern grouping are both good examples where clustering is appropriate.
Coincidentally, you will get the chance to work on both of these types of problems in this course.

Distance between two observations
  How similar / dis-similar are two observations
  Distance = 1 - similarity
  dist() function, take a dataframe
  
Calculate & plot the distance between two players

You've obtained the coordinates relative to the center of the field for two players in a soccer match and would like to calculate the distance between them.

In this exercise you will plot the positions of the 2 players and manually calculate the distance between them by using the euclidean distance formula.


```{r}
# create two_players data frame
two_players <- as.data.frame(rbind(c(5,4),c(15,10)))
three_players <- as.data.frame(rbind(c(5,4),c(15,10),c(0,20)))
four_players <- as.data.frame(rbind(c(5,4),c(15,10),c(0,20),c(-5,5)))
names(two_players) <- c("x","y");names(three_players) <- c("x","y");names(four_players) <- c("x","y")

```

```{r}
# Plot the positions of the players
ggplot(two_players, aes(x = x, y = y)) + 
  geom_point() +
  # Assuming a 40x60 field
  lims(x = c(-30,30), y = c(-20, 20))

# Split the players data frame into two observations
player1 <- two_players[1,]
player2 <- two_players[2,]

# Calculate and print their distance using the Euclidean Distance formula
# euclidean formula = sqrRoot of ((x1-x2)sqrd + (y1-y2)sqrd)
player_distance <- sqrt( (player1$x - player2$x)^2 + (player1$y - player2$y)^2 )
player_distance
```
Using the formula is a great way to learn how distance is measured between two observations. 

Using the dist() function

Using the euclidean formula manually may be practical for 2 observations but can get more complicated rather quickly when measuring the distance between many observations.

The dist() function simplifies this process by calculating distances between our observations (rows) using their features (columns). In this case the observations are the player positions and the dimensions are their x and y coordinates.

Note: The default distance calculation for the dist() function is euclidean distance
 

```{r}
# Calculate the Distance Between two_players
dist_two_players <- dist(two_players)
dist_two_players

# Calculate the Distance Between three_players
dist_three_players <- dist(three_players)
dist_three_players

dist_four_players <- dist(four_players)
dist_four_players
```
The dist() function makes life easier when working with many dimensions and observations. 

Effects of scale

You have learned that when a variable is on a larger scale than other variables in your data it may disproportionately influence the resulting distance calculated between your observations. Lets see this in action by observing a sample of data from the trees data set.

You will leverage the scale() function which by default centers & scales our column features.

Our variables are the following:

    Girth - tree diameter in inches
    Height - tree height in inches

> three_trees
  Girth Height
1   8.3    840
2   8.6    780
3  10.5    864
> 

```{r}
three_trees <- as.data.frame(rbind(c(8.3,840),c(8.6,780),c(10.5,864)))
# Calculate distance for three_trees 
dist_trees <- dist(three_trees)

# Scale three trees & calculate the distance  
scaled_three_trees <- scale(three_trees)
dist_scaled_trees <- dist(scaled_three_trees)

# Output the results of both Matrices
print('Without Scaling')
dist_trees

print('With Scaling')
dist_scaled_trees
```

Notice that before scaling observations 1 & 3 were the closest but after scaling observations 1 & 2 turn out to have the smallest distance. 

When to scale data?

Below are examples of datasets and their corresponding features.
In which of these examples would scaling **not** be necessary?

Taxi Trips - tip earned ($), distance traveled (km).
Health Measurements of Individuals - height (meters), weight (grams), body fat percentage (%).
Student Attributes - average test score (1-100), distance from school (km), annual household income ($).
Salespeople Commissions - total yearly commision ($), number of trips taken.
**Answer; None of the above, they all should be scaled when measuring distance.**

Euclidean is good for scaling continues data

Catagoral; binary (true/false)
Jaccard Index;
            A interect B
  J(A,B)  = -------------
            A union B
            
  Distance = 1 = simularity 
            J(1,2)= 1 - J(1,2) 
            
  dist(survey_a, method = "binary")

More Than Two Caegories

  Dummification in R

print(survey_b)  
  color sport
1  red   soccer
2  green hockey
3  blue  hockey
4  blue  soccer
library(dummies)

dummy.data.frame(survey_b)

  colorblue colorgreen colorred  sporthockey sportcoccer
1 0         0          1         0           1
2 0         1          0         1           0
3 1         0          0         1           0
4 1         0          0         0           1

dist(dummy_survey_b, method = 'binary') 

             1         2         3
2    1.0000000
3    1.0000000    0.666667
4    0.6666667    1.000000    0.66667


Calculating distance between categorical variables

In this exercise you will explore how to calculate binary (Jaccard) distances. In order to calculate distances we will first have to dummify our categories using the dummy.data.frame() from the library dummies

You will use a small collection of survey observations stored in the data frame job_survey with the following columns:

    job_satisfaction Possible options: "Hi", "Mid", "Low"
    is_happy Possible options: "Yes", "No"

> job_survey
  job_satisfaction is_happy
1              Low       No
2              Low       No
3               Hi      Yes
4              Low       No
5              Mid       No
> 

Instructions;
Create a dummified dataframe dummy_survey

Generate a Jaccard distance matrix for the dummified survey data dist_survey using the dist() function using the parameter method = 'binary'

Print the original data and the distance matrix
  Note the observations with a distance of 0 in the original data (1, 2, and 4)

> # Dummify the Survey Data
> dummy_survey <- dummy.data.frame(job_survey)
> 
> # Calculate the Distance
> dist_survey <- dist(dummy_survey, method = "binary")
> 
> # Print the Original Data
> job_survey
  job_satisfaction is_happy
1              Low       No
2              Low       No
3               Hi      Yes
4              Low       No
5              Mid       No
> 
> # Print the Distance Matrix
> dist_survey
          1         2         3         4
2 0.0000000                              
3 1.0000000 1.0000000                    
4 0.0000000 0.0000000 1.0000000          
5 0.6666667 0.6666667 1.0000000 0.6666667

Notice that this distance metric successfully captured that observations 1 and 2 are identical (distance of 0) 

2 Hierarchical clustering

  This chapter will help you answer the last question from chapter 1 - how do you find groups of similar observations (clusters) in your data using the distances that you have calculated? You will learn about the fundamental principles of hierarchical clustering - the linkage criteria and the dendrogram plot - and how both are used to build clusters. You will also explore data from a wholesale distributor in order to perform market segmentation of clients using their spending habits.
  
Linkage Cirteria

Complete Linkage: maximum distance between two sets
Sigle Linkage: minimum distance between two sets
Average Linkage: average distance between two sets

Calculating linkage

Let us revisit the example with three players on a field. The distance matrix between these three players is shown below and is available as the variable dist_players.

From this we can tell that the first group that forms is between players 1 & 2, since they are the closest to one another with a euclidean distance value of 11.

Now you want to apply the three linkage methods you have learned to determine what the distance of this group is to player 3.

> dist_players
         1        2
2 11.66190         
3 16.76305 18.02776
> 

  
```{r}
# Extract the pair distances
distance_1_2 <- 11.66190 # dist_players[1]
distance_1_3 <- 16.76305 # dist_players[2]
distance_2_3 <- 18.02776 # dist_players[3]

# Calculate the complete distance between group 1-2 and 3
max(c(distance_1_3, distance_2_3))

# Calculate the single distance between group 1-2 and 3
min(c(distance_1_3, distance_2_3))

# Calculate the average distance between group 1-2 and 3
mean(c(distance_1_3, distance_2_3))

```
Now you have all the knowledge you need to tackle exercise 12 from chapter 1. 

> dist_players
         1        2        3
2 11.66190                  
3 16.76305 18.02776         
4 10.04988 20.61553 15.81139
> 

Complete Linkage: Player 3,
Single & Average Linkage: Player 2
you can see that the choice of the linkage method can drastically change the result of this question.

```{r}
# load table from CSV
  inFileName <- "players"
  filePath <- "./data/"
  fileName <- paste(inFileName, ".csv", sep = "")
  players <- read_csv(paste(filePath, fileName, sep = ""))
```

players <- load players.csv

dist_players <- dist(players, method = 'euclidean')
hc_players <- hclust(dist_players, method = 'complete')
cluster_assignments <- cutree(hc_players, k=2)

players_clustered <- mutate(players, cluster = cluster_assignments)

plot and color points by cluster
library(ggplot2)
ggplot(players_clustered, aes(x=x, y=y, color = factor(cluster))) +
  geom_point()
  
Assign cluster membership

In this exercise you will leverage the hclust() function to calculate the iterative linkage steps and you will use the cutree() function to extract the cluster assignments for the desired number (k) of clusters.

You are given the positions of 12 players at the start of a 6v6 soccer match. This is stored in the lineup dataframe.

You know that this match has two teams (k = 2), let's use the clustering methods you learned to assign which team each player belongs in based on their position.

Notes:

    The linkage method can be passed via the method parameter: hclust(distance_matrix, method = "complete")
    Remember that in soccer opposing teams start on their half of the field.
    Because these positions are measured using the same scale we do not need to re-scale our data.
    

```{r}
# load linup from sav file
library(readr)
con <- gzfile("./data/cluster_lineup.rds")
lineup <- readRDS(con)
close(con)
```


```{r}
library(dplyr)
# Calculate the Distance
dist_players <- dist(lineup, method = "euclidean")

# Perform the hierarchical clustering using the complete linkage
hc_players <- hclust(dist_players, method = 'complete')

# Calculate the assignment vector with a k of 2
clusters_k2 <- cutree(hc_players, k=2)

# Create a new dataframe storing these results
lineup_k2_complete <- mutate(lineup, cluster = clusters_k2)
```
  
Exploring the clusters

Because clustering analysis is always in part qualitative, it is incredibly important to have the necessary tools to explore the results of the clustering.

In this exercise you will explore that data frame you created in the previous exercise lineup_k2_complete.

Reminder: The lineup_k2_complete dataframe contains the x & y positions of 12 players at the start of a 6v6 soccer game to which you have added clustering assignments based on the following parameters:

    Distance: Euclidean
    Number of Clusters (k): 2
    Linkage Method: Complete

```{r}
# Count the cluster assignments
count(lineup_k2_complete, cluster)

# Plot the positions of the players and color them using their cluster
library(ggplot2)
ggplot(lineup_k2_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()
```

Think carefully about whether these results make sense to you and why. 

Comparing average, single & complete linkage

You are now ready to analyze the clustering results of the lineup dataset using the dendrogram plot. This will give you a new perspective on the effect the decision of the linkage method has on your resulting cluster analysis.

```{r}
# Prepare the Distance Matrix
dist_players <- dist(lineup)

# Generate hclust for complete, single & average linkage methods
hc_complete <- hclust(dist_players,method = 'complete')
hc_single <- hclust(dist_players,method = 'single')
hc_average <- hclust(dist_players,method = 'average')

# Plot & Label the 3 Dendrograms Side-by-Side
# Hint: To see these Side-by-Side run the 4 lines together as one command
par(mfrow = c(1,3))
plot(hc_complete, main = 'Complete Linkage')
plot(hc_single, main = 'Single Linkage')
plot(hc_average, main = 'Average Linkage')
```

Did you notice how the trees all look different?
In the coming exercises you will see how visualizing this structure can be helpful for building clusters. 

Height of the tree

An advantage of working with a clustering method like hierarchical clustering is that you can describe the relationships between your observations based on both the distance metric and the linkage metric selected (the combination of which defines the height of the tree).


Based on the code below what can you concretely say about the height of a branch in the resulting dendrogram?

```{r}
dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = 'single')
plot(hc_players) 
```

a minimum euclidean distance amongst each other less than or equal to the height of the branch.

Exactly! Based on this code we can concretely say that for a given branch on a tree all members that are a part of that branch must have a minimum euclidean distance amongst one another equal to or less than the height of that branch.
In the next section you will see how this description can be put into action to generate clusters that can be described using the same logic.

```{r}
# done; install.packages("dendextend")
library(dendextend);library(dplyr)
dend_players <- as.dendrogram(hc_players)
dend_colored <- color_branches(dend_players, h=10) # or k=xx
plot(dend_colored)
cluster_assignments <- cutree(hc_players, h = 15)
print(cluster_assignments)
library(dplyr)
players_clustered <- mutate(hc_players, cluster = cluster_assignments)
print(players_clustered)
```

Clusters based on height

In previous exercises you have grouped your observations into clusters using a pre-defined number of clusters (k). In this exercise you will leverage the visual representation of the dendrogram in order to group your observations into clusters using a maximum height (h), below which clusters form.

You will work the color_branches() function from the dendextend library in order to visually inspect the clusters that form at any height along the dendrogram.

The hc_players has been carried over from your previous work with the soccer line-up data.

```{r}
library(dendextend)
dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = "complete")

# Create a dendrogram object from the hclust variable
dend_players <- as.dendrogram(hc_players)

# Plot the dendrogram
plot(dend_players)

# Color branches by cluster formed from the cut at a height of 20 & plot
dend_20 <- color_branches(dend_players, h = 20)

# Plot the dendrogram with clusters colored below height 20
plot(dend_20)

# Color branches by cluster formed from the cut at a height of 40 & plot
dend_40 <- color_branches(dend_players, h = 40)

# Plot the dendrogram with clusters colored below height 40
plot(dend_40)

```
Can you see that the height that you use to cut the tree greatly influences the number of clusters and their size? Consider taking a moment to play with other values of height before continuing. 

Exploring the branches cut from the tree

The cutree() function you used in exercises 5 & 6 can also be used to cut a tree at a given height by using the h parameter. Take a moment to explore the clusters you have generated from the previous exercises based on the heights 20 & 40.

```{r}
dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = "complete")

# Calculate the assignment vector with a h of 20
clusters_h20 <- cutree(hc_players, h = 20)

# Create a new dataframe storing these results
lineup_h20_complete <- mutate(lineup, cluster = clusters_h20)

# Calculate the assignment vector with a h of 40
clusters_h40 <- cutree(hc_players, h = 40)

# Create a new dataframe storing these results
lineup_h40_complete <- mutate(lineup, cluster = clusters_h40)

# Plot the positions of the players and color them using their cluster for height = 20
ggplot(lineup_h20_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()

# Plot the positions of the players and color them using their cluster for height = 40
ggplot(lineup_h40_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()

```
You can now explore your clusters using both k and h parameters.


What do we know about our clusters?

Based on the code below, what can you concretely say about the relationships of the members within each cluster?
```{r}
dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = 'complete')
clusters <- cutree(hc_players, h = 40) 
```
Every member belonging to a cluster must have:

a maximum euclidean distance to all other members of its cluster that is less than 40.

The height of any branch is determined by the linkage and distance decisions (in this case complete linkage and euclidean distance). While the members of the clusters that form below a desired height have a maximum linkage+distance amongst themselves that - han the desired height.

Segment wholesale customers

You're now ready to use hierarchical clustering to perform market segmentation (i.e. use consumer characteristics to group them into subgroups).

In this exercise you are provided with the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the dataframe customers_spend. Assign these clients into meaningful clusters.

Note: For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.


```{r}
con <- gzfile("./data/cluster_ws_customers.rds")
customers_spend <- readRDS(con)
close(con)

```


Segment wholesale customers

You're now ready to use hierarchical clustering to perform market segmentation (i.e. use consumer characteristics to group them into subgroups).

In this exercise you are provided with the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the dataframe customers_spend. Assign these clients into meaningful clusters.

Note: For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.

```{r}
# Calculate euclidean distance between customers
dist_customers <- dist(customers_spend, method = 'euclidean')

# Generate a complete linkage analysis 
hc_customers <- hclust(dist_customers, method = "complete")

# Plot the dendrogram
plot(hc_customers)

# Create a cluster assignment vector at h = 15000
clust_customers <- cutree(hc_customers, h = 15000)

# Generate the segmented customers dataframe
segment_customers <- mutate(customers_spend, cluster = clust_customers)

```
Let's move on to the next exercise and explore these clusters. 

```{r}
dist_customers <- dist(customers_spend)
hc_customers <- hclust(dist_customers)
clust_customers <- cutree(hc_customers, h = 15000)
segment_customers <- mutate(customers_spend, cluster = clust_customers)

# Count the number of customers that fall into each cluster
count(segment_customers, cluster)

# Color the dendrogram based on the height cutoff
dend_customers <- as.dendrogram(hc_customers)
dend_colored <- color_branches(dend_customers, h = 15000)

# Plot the colored dendrogram
plot(dend_colored)

# Calculate the mean for each category
segment_customers %>% 
  group_by(cluster) %>% 
  summarise_all(funs(mean(.)))
```
You've gathered a bunch of information about these clusters, now let's see what can be interpreted from them. 

Interpreting the wholesale customer clusters

What observations can we make about our segments based on their average spending in each category?

Customers in cluster 1 spent more money on Milk than any other cluster.

Customers in cluster 3 spent more money on Grocery than any other cluster.

Customers in cluster 4 spent more money on Frozen goods than any other cluster.

The majority of customers fell into cluster 2 and did not show any excessive spending in any category.

**All of the above.**

All 4 statements are reasonable, but whether they are meaningful depends heavily on the business context of the clustering.


3 K-means clustering

  In this chapter, you will build an understanding of the principles behind the k-means algorithm, learn how to select the right k when it isn't previously known, and revisit the wholesale data from a different perspective. 
  
  centroids only, eculidian only


```{r}
con <- gzfile("./data/cluster_lineup.rds")
lineup <- readRDS(con)
close(con)
```

```{r}
library(dplyr)
model <- kmeans(lineup, centers = 2)
print(model$cluster)
lineup_clustered <- mutate(lineup, cluster = model$cluster)
print(lineup_clustered)
```

K-means on a soccer field

In the previous chapter you used the lineup dataset to learn about hierarchical clustering, in this chapter you will use the same data to learn about k-means clustering. As a reminder, the lineup dataframe contains the positions of 12 players at the start of a 6v6 soccer match.

Just like before, you know that this match has two teams on the field so you can perform a k-means analysis using k = 2 in order to determine which player belongs to which team.

Note that in the kmeans() function k is specified using the centers parameter.

```{r}
library(dendextend);library(ggplot2)
# Build a kmeans model
model_km2 <- kmeans(lineup, centers = 2)

# Extract the cluster assignment vector from the kmeans model
clust_km2 <- model_km2$cluster

# Create a new dataframe appending the cluster assignment
lineup_km2 <- mutate(lineup, cluster = clust_km2)

# Plot the positions of the players and color them using their cluster
ggplot(lineup_km2, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()
```

Knowing the desired number of clusters ahead of time can be very helpful when performing a k-means analysis. In the next section we will see what happens when we use an incorrect value of k. 

K-means on a soccer field (part 2)

In the previous exercise you successfully used the k-means algorithm to cluster the two teams from the lineup data frame. This time, let's explore what happens when you use a k of 3.

You will see that the algorithm will still run, but does it actually make sense in this context...

```{r}
# Build a kmeans model
model_km3 <- kmeans(lineup, centers = 3)

# Extract the cluster assignment vector from the kmeans model
clust_km3 <- model_km3$cluster

# Create a new dataframe appending the cluster assignment
lineup_km3 <- mutate(lineup, cluster = clust_km3)

# Plot the positions of the players and color them using their cluster
ggplot(lineup_km3, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()
```

Does this result make sense? **Remember we only have 2 teams on the field**. It's very important to remember that k-means will run with any k that is more than 2 and less than your total observations, but it doesn't always mean the results will be meaningful. 

```{r}
  # 1 - calculate within cluster sum of squares
    model@tot.withinss # (from kmeans())
  #2 - do this for mulitble values, more models
    library(purrr);library(dplyr);library(dendextend);library(ggplot2)
    tot_withinss <- map_dbl(1:10, function(k) {
      model <- kmeans(x = lineup, centers = k)
      model$.withinss
    })
  elbow_df <- data.frame (
    k = 1:10,
    tot_withinss = tot_withinss
  )    
  print(elbow_df)
  
  ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
    geom_line() +
    scale_x_continuous(breaks = 1:10)
```

Many K's many models

While the lineup dataset clearly has a known value of k, often times the optimal number of clusters isn't known and must be estimated.

In this exercise you will leverage map_dbl() from the purrr library to run k-means using values of k ranging from 1 to 10 and extract the total within-cluster sum of squares metric from each one. This will be the first step towards visualizing the elbow plot.

```{r}
library(purrr)

# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = lineup, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10 ,
  tot_withinss = tot_withinss
)
```

In the next exercise you will plot the elbow plot from this data. 

```{r}
# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = lineup, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)
```
You have learned how to create and visualize elbow plots as a tool for finding a “good” value of k. In the next section you will add another tool to your arsenal for finding k.

Correct Elbow = 2 
You can see that there is a sharp change in the slope of this line that makes an \"elbow\" shape. Furthermore, this is supported by the prior knowledge that there are two teams in this data and a k of 2 is desired.

Silhouette Analysis Method

Silhouette Width
  within Cluster Distance
  Closest Neighbor Distance
  
  between -1 ----------------------- 0 ------------------------- 1
    Better fit in neighbor      on boarder                Well matched to cluster
    
```{r}
library(cluster)
pam_k3 <- pam(lineup, k=3)
pam_k3$silinfo
sil_plot <- silhouette(pam_k3)
plot(sil_plot)
pam_k3$silinfo$avg.width
```

```{r}
sil_width <- map_dbl(2:10,  function(k){
  model <- pam(x = lineup, k = k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and tot_withinss
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the elbow plot
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  scale_x_continuous(breaks = 2:10)
```
Highest vale is the recommended one (2 in this case, left most)

Silhouette analysis

Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:

    Values close to 1 suggest that the observation is well matched to the assigned cluster
    Values close to 0 suggest that the observation is borderline matched between two clusters
    Values close to -1 suggest that the observations may be assigned to the wrong cluster

In this exercise you will leverage the pam() and the silhouette() functions from the cluster library to perform silhouette analysis to compare the results of models with a k of 2 and a k of 3. You'll continue working with the lineup dataset.

```{r}
library(cluster)

# Generate a k-means model using the pam() function with a k = 2
pam_k2 <- pam(lineup, k = 2)

# Plot the silhouette visual for the pam_k2 model
plot(silhouette(pam_k2))

# Generate a k-means model using the pam() function with a k = 3
pam_k3 <- pam(lineup, k = 3)

# Plot the silhouette visual for the pam_k3 model
plot(silhouette(pam_k3))

```
Make sure to review the differences between the plots before proceeding (especially observation 3) for pam_k3.

Did you notice that for k = 2, no observation has a silhouette width close to 0? What about the fact that for k = 3, observation 3 is close to 0 and is negative? This suggests that k = 3 is NOT the right number of clusters. 

Making sense of the K-means clusters

```{r}
con <- gzfile("./data/cluster_ws_customers.rds")
customers_spend <- readRDS(con)
close(con)
customer_spend
```

Estimate the "best" k using average silhouette width
Run k-means with the suggested k
Characterize the spending habits of these clusters of customers.

Revisiting wholesale data: "Best" k

At the end of Chapter 2 you explored wholesale distributor data customers_spend using hierarchical clustering. This time you will analyze this data using the k-means clustering tools covered in this chapter.

The first step will be to determine the "best" value of k using average silhouette width.

A refresher about the data: it contains records of the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the dataframe customers_spend. For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.

```{r}
# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
  model <- pam(x = customers_spend, k = k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and sil_width
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the relationship between k and sil_width
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  scale_x_continuous(breaks = 2:10)
```

From the plot I hope you noticed that k = 2 has the highest average sillhouette width and is the "best" value of k we will move forward with. 

Revisiting wholesale data: Exploration

From the previous analysis you have found that k = 2 has the highest average silhouette width. In this exercise you will continue to analyze the wholesale customer data by building and exploring a kmeans model with 2 clusters.

```{r}
set.seed(42)

# Build a k-means model for the customers_spend with a k of 2
model_customers <- kmeans(customers_spend, centers = 2)

# Extract the vector of cluster assignments from the model
clust_customers <- model_customers$cluster

# Build the segment_customers dataframe
segment_customers <- mutate(customers_spend, cluster = clust_customers)

# Calculate the size of each cluster
count(segment_customers, cluster)

# Calculate the mean for each category
segment_customers %>% 
  group_by(cluster) %>% 
  summarise_all(funs(mean(.)))
```

It seems that in this case cluster 1 consists of individuals who proportionally spend more on Frozen food while cluster 2 customers spend more on Milk and Grocery. Did you notice that when you explored this data using hierarchical clustering, the method resulted in 4 clusters while using k-means got you 2? Both of these results are valid, but which one is appropriate for this would require more subject matter expertise. 

Before you proceed with the next chapter, remember that: 
  **Generating clusters is a science, but interpreting them is an art. **

4 Case Study: National Occupational mean wage

  In this chapter, you will apply the skills you have learned to explore how the average salary amongst professions have changed over time. 
  
  good for spacial data; segment customers into cluster
  this one.... time series

```{r}
con <- gzfile("./data/cluster_oes.rds")
oes <- readRDS(con)
close(con)
```

Occupational Wage Data
  22 Occupation Observations
  15 Measurements of Average Income from 2001-2016
  
Which occupations cluster together?  
Are there trends that we can observe?

Steps; Hoerarchical Clustering
      
      Evaluate whether preprocessing is necessary
      create a distance matrix
      build a dendrogram
      extracting clusters from dendrogram
      explore resulting clusters


Initial exploration of the data <------------------------

You are presented with data from the Occupational Employment Statistics (OES) program which produces employment and wage estimates annually. This data contains the yearly average income from 2001 to 2016 for 22 occupation groups. You would like to use this data to identify clusters of occupations that maintained similar income trends.

The data is stored in your environment as the data.matrix oes.

Before you begin to cluster this data you should determine whether any pre-processing steps (such as scaling and imputation) are necessary.

Leverage the functions head() and summary() to explore the oes data in order to determine which of the pre-processing steps below are necessary:

```{r}
head(oes)
summary(oes)
```

there are no missing values, no categorical and the features are on the same scale.
Now you're ready to cluster this data! 

Hierarchical clustering: Occupation trees <-------------------

In the previous exercise you have learned that the oes data is ready for hierarchical clustering without any preprocessing steps necessary. In this exercise you will take the necessary steps to build a dendrogram of occupations based on their yearly average salaries and propose clusters using a height of 100,000.

```{r}
# Calculate euclidean distance between the occupations
dist_oes <- dist(oes, method = "euclidean")

# Generate an average linkage analysis 
hc_oes <- hclust(dist_oes, method = "average")

# Create a dendrogram object from the hclust variable
dend_oes <- as.dendrogram(hc_oes)

# Plot the dendrogram
plot(dend_oes)

# Color branches by cluster formed from the cut at a height of 100000
dend_colored <- color_branches(dend_oes, h = 100000)

# Plot the colored dendrogram
plot(dend_colored)
```
Based on the dendrogram it may be reasonable to start with the three clusters formed at a height of 100,000. The members of these clusters appear to be tightly grouped but different from one another. Let's continue this exploration. 

Hierarchical clustering: Preparing for exploration <-----------------

You have now created a potential clustering for the oes data, before you can explore these clusters with ggplot2 you will need to process the oes data matrix into a tidy data frame with each occupation assigned its cluster.

```{r}
dist_oes <- dist(oes, method = 'euclidean')
hc_oes <- hclust(dist_oes, method = 'average')

library(tibble)
library(tidyr)

# Use rownames_to_column to move the rownames into a column of the data frame
df_oes <- rownames_to_column(as.data.frame(oes), var = 'occupation')

# Create a cluster assignment vector at h = 100,000
cut_oes <- cutree(hc_oes, h = 100000)

# Generate the segmented the oes dataframe
clust_oes <- mutate(df_oes, cluster = cut_oes)

# Create a tidy data frame by gathering the year and values into two columns
gathered_oes <- gather(data = clust_oes, 
                       key = year, 
                       value = mean_salary, 
                       -occupation, -cluster)

gathered_oes
```

You now have the dataframes necessary to explore the results of this clustering 

Hierarchical clustering: Plotting occupational clusters <----------------

You have succesfully created all the parts necessary to explore the results of this hierarchical clustering work. In this exercise you will leverage the named assignment vector cut_oes and the tidy data frame gathered_oes to analyze the resulting clusters.

```{r}
# View the clustering assignments by sorting the cluster assignment vector
sort(cut_oes)

# Plot the relationship between mean_salary and year and color the lines by the assigned cluster
ggplot(gathered_oes, aes(x = year, y = mean_salary, color = factor(cluster))) + 
    geom_line(aes(group = occupation))
```
Cool huh! From this work it looks like both Management & Legal professions (cluster 1) experienced the most rapid growth in these 15 years. Let's see what we can get by exploring this data using k-means. 

Next Steps: k-means Clustering <---------------
    Estimate whether preprocessing is necessary
    estimate the best K using the elbow plot
    estimate the best K using the maximum average silhouette width
    explore resulting clusters

K-means: Elbow analysis <--------------------------

In the previous exercises you used the dendrogram to propose a clustering that generated 3 trees. In this exercise you will leverage the k-means elbow plot to propose the "best" number of clusters.

```{r}
# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = oes, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)
```
Fascinating! So the elbow analysis proposes a different value of k, in the next section let's see what we can learn from Silhouette Width Analysis.

K-means: Average Silhouette Widths <------------------

So hierarchical clustering resulting in 3 clusters and the elbow method suggests 2. In this exercise use average silhouette widths to explore what the "best" value of k should be.

```{r}
# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
  model <- pam(oes, k = k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and sil_width
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the relationship between k and sil_width
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  scale_x_continuous(breaks = 2:10)
```
It seems that this analysis results in another value of k, this time 7 is the top contender (although 2 comes very close). 

change this for graph 2 and 3
```{r}
# Generate the segmented the oes dataframe
clust_oes <- mutate(df_oes, cluster = cut_oes)

# Create a tidy data frame by gathering the year and values into two columns
gathered_oes <- gather(data = clust_oes, 
                       key = year, 
                       value = mean_salary, 
                       -occupation, -cluster)
gathered_oes

# graph2
# Generate the segmented the oes dataframe
clust_oes <- mutate(df_oes, cluster = cut_oes)

# Create a tidy data frame by gathering the year and values into two columns
gathered_oes <- gather(data = clust_oes, 
                       key = year, 
                       value = mean_salary, 
                       -occupation, -cluster)
gathered_oes

# graph3

```


```{r}
# Hierarchical Cluster
ggplot(gathered_oes, aes(x = year, y = mean_salary, color = factor(cluster))) + 
    geom_line(aes(group = occupation))


# Hierarchical Cluster
ggplot(elbow_df, aes(x = k, y = tot_withinss, color = factor(k))) + 
    geom_line(aes(group = k))

# Hierarchical Cluster
ggplot(sil_df, aes(x = k, y = sil_width, color = factor(k))) + 
    geom_line(aes(group = k))

```

All 3 statements are correct but there is no quantitative way to determine which of these clustering approaches is the right one without futher exploration.

Hierarchical Clustering; better for not huges data sets

                                Hierarchical Cluster            k-means
Distance used:                    virtually any                 euclidean only
results stable:                   Yes                           No
Evaluating # of Clusters;     dendorogram, silhouette, elbow    silhouette, elbo
Computatin Complexity:        relatively Higher                 relatively lower


more tools for cluster;
  k-mediods
  DBSCAN
  Optics
  









